<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
  <title>Towards Comprehensive Reasoning in Vision-Language Models | ICCV 2025</title>
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css" />
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Audiowide&family=Crimson+Pro:wght@300;400;600;700&family=DM+Mono:wght@400;500&display=swap" rel="stylesheet">
  <style>
    :root {
      --primary: #00d9ff;
      --primary-dark: #0099cc;
      --secondary: #ff3366;
      --accent: #ffcc00;
      --dark: #0a0e27;
      --dark-alt: #151932;
      --dark-card: #1a1f3a;
      --text: #e0e6ed;
      --text-muted: #8892ab;
      --gradient-1: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
      --gradient-2: linear-gradient(135deg, #f093fb 0%, #f5576c 100%);
      --gradient-3: linear-gradient(135deg, #4facfe 0%, #00f2fe 100%);
      --glow: 0 0 20px rgba(0, 217, 255, 0.3);
    }

    * {
      margin: 0;
      padding: 0;
      box-sizing: border-box;
    }

    body {
      font-family: 'Crimson Pro', serif;
      background: var(--dark);
      color: var(--text);
      line-height: 1.6;
      overflow-x: hidden;
    }

    /* Animated background */
    .bg-canvas {
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      height: 100%;
      z-index: 0;
      opacity: 0.4;
    }

    /* Navigation */
    nav {
      position: fixed;
      top: 0;
      width: 100%;
      background: rgba(10, 14, 39, 0.95);
      backdrop-filter: blur(20px);
      border-bottom: 1px solid rgba(0, 217, 255, 0.1);
      z-index: 1000;
      padding: 1.2rem 0;
      transition: all 0.3s ease;
    }

    nav.scrolled {
      padding: 0.8rem 0;
      box-shadow: 0 10px 40px rgba(0, 0, 0, 0.3);
    }

    .nav-container {
      max-width: 1400px;
      margin: 0 auto;
      padding: 0 2rem;
      display: flex;
      justify-content: space-between;
      align-items: center;
    }

    .logo {
      font-family: 'Audiowide', cursive;
      font-size: 1.5rem;
      background: var(--gradient-3);
      -webkit-background-clip: text;
      -webkit-text-fill-color: transparent;
      background-clip: text;
      letter-spacing: 2px;
      display: flex;
      align-items: center;
      gap: 12px;
    }

    .logo-icon {
      width: 40px;
      height: 40px;
      background: var(--gradient-3);
      border-radius: 8px;
      display: flex;
      align-items: center;
      justify-content: center;
      color: white;
      font-weight: bold;
      box-shadow: var(--glow);
    }

    .nav-links {
      display: flex;
      gap: 2.5rem;
      list-style: none;
      align-items: center;
    }

    .nav-links a {
      color: var(--text);
      text-decoration: none;
      font-weight: 500;
      font-size: 1.05rem;
      position: relative;
      transition: color 0.3s ease;
      font-family: 'DM Mono', monospace;
      letter-spacing: 0.5px;
    }

    .nav-links a::after {
      content: '';
      position: absolute;
      bottom: -5px;
      left: 0;
      width: 0;
      height: 2px;
      background: var(--primary);
      transition: width 0.3s ease;
      box-shadow: var(--glow);
    }

    .nav-links a:hover {
      color: var(--primary);
    }

    .nav-links a:hover::after {
      width: 100%;
    }

    .zoom-btn {
      background: var(--gradient-3);
      color: white;
      padding: 0.7rem 1.5rem;
      border-radius: 25px;
      text-decoration: none;
      font-weight: 600;
      transition: all 0.3s ease;
      box-shadow: 0 5px 20px rgba(0, 217, 255, 0.3);
      font-family: 'DM Mono', monospace;
      font-size: 0.95rem;
    }

    .zoom-btn:hover {
      transform: translateY(-2px);
      box-shadow: 0 8px 30px rgba(0, 217, 255, 0.5);
    }

    /* Mobile menu */
    .menu-toggle {
      display: none;
      flex-direction: column;
      gap: 5px;
      cursor: pointer;
      z-index: 1001;
    }

    .menu-toggle span {
      width: 25px;
      height: 3px;
      background: var(--primary);
      border-radius: 3px;
      transition: all 0.3s ease;
    }

    /* Hero Section */
    .hero {
      min-height: 100vh;
      display: flex;
      align-items: center;
      justify-content: center;
      position: relative;
      padding: 8rem 2rem 4rem;
      overflow: hidden;
    }

    .hero-content {
      max-width: 1200px;
      text-align: center;
      position: relative;
      z-index: 1;
      animation: fadeInUp 1s ease;
    }

    @keyframes fadeInUp {
      from {
        opacity: 0;
        transform: translateY(30px);
      }
      to {
        opacity: 1;
        transform: translateY(0);
      }
    }

    .hero h1 {
      font-size: clamp(2.5rem, 6vw, 5rem);
      font-weight: 700;
      margin-bottom: 1.5rem;
      line-height: 1.1;
      background: linear-gradient(135deg, #ffffff 0%, var(--primary) 100%);
      -webkit-background-clip: text;
      -webkit-text-fill-color: transparent;
      background-clip: text;
      animation: fadeInUp 1s ease 0.2s both;
    }

    .hero .subtitle {
      font-family: 'Audiowide', cursive;
      font-size: clamp(1.2rem, 3vw, 2rem);
      color: var(--primary);
      margin-bottom: 2rem;
      letter-spacing: 3px;
      text-transform: uppercase;
      animation: fadeInUp 1s ease 0.4s both;
    }

    .hero-meta {
      display: flex;
      gap: 3rem;
      justify-content: center;
      align-items: center;
      margin-top: 3rem;
      flex-wrap: wrap;
      animation: fadeInUp 1s ease 0.6s both;
    }

    .hero-meta-item {
      display: flex;
      align-items: center;
      gap: 12px;
      font-size: 1.15rem;
      color: var(--text-muted);
      font-family: 'DM Mono', monospace;
    }

    .hero-meta-item i {
      color: var(--primary);
      font-size: 1.3rem;
    }

    /* Section Styles */
    section {
      position: relative;
      z-index: 1;
      padding: 6rem 2rem;
    }

    .container {
      max-width: 1400px;
      margin: 0 auto;
    }

    .section-title {
      font-size: clamp(2.5rem, 5vw, 4rem);
      font-weight: 700;
      margin-bottom: 1rem;
      background: linear-gradient(135deg, var(--primary) 0%, var(--secondary) 100%);
      -webkit-background-clip: text;
      -webkit-text-fill-color: transparent;
      background-clip: text;
      display: inline-block;
    }

    .section-subtitle {
      font-size: 1.2rem;
      color: var(--text-muted);
      margin-bottom: 4rem;
      font-family: 'DM Mono', monospace;
    }

    /* Introduction */
    .intro-text {
      font-size: 1.25rem;
      line-height: 2;
      color: var(--text);
      background: var(--dark-card);
      padding: 3rem;
      border-radius: 20px;
      border: 1px solid rgba(0, 217, 255, 0.1);
      box-shadow: 0 10px 40px rgba(0, 0, 0, 0.3);
      position: relative;
      overflow: hidden;
    }

    .intro-text::before {
      content: '';
      position: absolute;
      top: 0;
      left: 0;
      width: 5px;
      height: 100%;
      background: var(--gradient-3);
    }

    /* Schedule Table */
    .schedule-container {
      background: var(--dark-card);
      border-radius: 20px;
      overflow: hidden;
      border: 1px solid rgba(0, 217, 255, 0.1);
      box-shadow: 0 10px 40px rgba(0, 0, 0, 0.3);
    }

    .schedule-item {
      padding: 2rem;
      border-bottom: 1px solid rgba(255, 255, 255, 0.05);
      transition: all 0.3s ease;
      display: grid;
      grid-template-columns: 150px 1fr 200px;
      gap: 2rem;
      align-items: start;
    }

    .schedule-item:hover {
      background: rgba(0, 217, 255, 0.05);
      border-left: 4px solid var(--primary);
    }

    .schedule-item:last-child {
      border-bottom: none;
    }

    .schedule-time {
      font-family: 'Audiowide', cursive;
      font-size: 1.1rem;
      color: var(--primary);
      font-weight: 600;
      letter-spacing: 1px;
    }

    .schedule-content h3 {
      font-size: 1.4rem;
      margin-bottom: 0.5rem;
      color: var(--text);
    }

    .schedule-speaker {
      color: var(--text-muted);
      font-size: 1.1rem;
      font-family: 'DM Mono', monospace;
    }

    .schedule-speaker a {
      color: var(--primary);
      text-decoration: none;
      transition: color 0.3s ease;
    }

    .schedule-speaker a:hover {
      color: var(--secondary);
    }

    .schedule-actions {
      display: flex;
      gap: 1rem;
      align-items: center;
      justify-content: flex-end;
    }

    .btn-abstract, .btn-slides {
      padding: 0.6rem 1.2rem;
      border-radius: 8px;
      text-decoration: none;
      font-size: 0.95rem;
      transition: all 0.3s ease;
      cursor: pointer;
      font-family: 'DM Mono', monospace;
      border: none;
      background: none;
    }

    .btn-abstract {
      color: var(--primary);
      border: 1px solid var(--primary);
    }

    .btn-abstract:hover {
      background: var(--primary);
      color: var(--dark);
      box-shadow: var(--glow);
    }

    .btn-slides {
      background: var(--gradient-3);
      color: white;
      box-shadow: 0 5px 15px rgba(0, 217, 255, 0.3);
    }

    .btn-slides:hover {
      transform: translateY(-2px);
      box-shadow: 0 8px 25px rgba(0, 217, 255, 0.5);
    }

    .abstract-content {
      grid-column: 1 / -1;
      margin-top: 1.5rem;
      padding: 2rem;
      background: rgba(0, 217, 255, 0.05);
      border-radius: 12px;
      border-left: 3px solid var(--primary);
      display: none;
      animation: slideDown 0.3s ease;
    }

    .abstract-content.active {
      display: block;
    }

    @keyframes slideDown {
      from {
        opacity: 0;
        transform: translateY(-10px);
      }
      to {
        opacity: 1;
        transform: translateY(0);
      }
    }

    /* People Cards */
    .people-grid {
      display: grid;
      grid-template-columns: repeat(auto-fit, minmax(350px, 1fr));
      gap: 2.5rem;
      margin-top: 3rem;
    }

    .person-card {
      background: var(--dark-card);
      border-radius: 20px;
      padding: 2.5rem;
      border: 1px solid rgba(0, 217, 255, 0.1);
      transition: all 0.4s ease;
      position: relative;
      overflow: hidden;
    }

    .person-card::before {
      content: '';
      position: absolute;
      top: 0;
      left: 0;
      width: 100%;
      height: 4px;
      background: var(--gradient-3);
      transform: scaleX(0);
      transform-origin: left;
      transition: transform 0.4s ease;
    }

    .person-card:hover {
      transform: translateY(-10px);
      box-shadow: 0 20px 60px rgba(0, 217, 255, 0.2);
      border-color: var(--primary);
    }

    .person-card:hover::before {
      transform: scaleX(1);
    }

    .person-header {
      display: flex;
      gap: 2rem;
      margin-bottom: 1.5rem;
    }

    .person-avatar {
      width: 120px;
      height: 120px;
      border-radius: 50%;
      border: 3px solid var(--primary);
      box-shadow: 0 5px 20px rgba(0, 217, 255, 0.3);
      object-fit: cover;
      flex-shrink: 0;
    }

    .person-info h3 {
      font-size: 1.6rem;
      margin-bottom: 0.5rem;
      color: var(--text);
    }

    .person-affiliation {
      color: var(--text-muted);
      font-size: 1.05rem;
      font-family: 'DM Mono', monospace;
    }

    .person-bio {
      color: var(--text-muted);
      line-height: 1.8;
      margin-top: 1.5rem;
      font-size: 1.05rem;
    }

    .person-links {
      display: flex;
      gap: 1.2rem;
      margin-top: 1.5rem;
    }

    .person-links a {
      width: 40px;
      height: 40px;
      border-radius: 50%;
      background: rgba(0, 217, 255, 0.1);
      display: flex;
      align-items: center;
      justify-content: center;
      color: var(--primary);
      text-decoration: none;
      transition: all 0.3s ease;
      font-size: 1.2rem;
    }

    .person-links a:hover {
      background: var(--primary);
      color: var(--dark);
      transform: translateY(-3px);
      box-shadow: var(--glow);
    }

    /* Gallery */
    .gallery-grid {
      display: grid;
      grid-template-columns: repeat(auto-fill, minmax(300px, 1fr));
      gap: 1.5rem;
      margin-top: 3rem;
    }

    .gallery-item {
      position: relative;
      border-radius: 15px;
      overflow: hidden;
      aspect-ratio: 4/3;
      cursor: pointer;
      transition: all 0.4s ease;
      border: 2px solid rgba(0, 217, 255, 0.1);
    }

    .gallery-item img {
      width: 100%;
      height: 100%;
      object-fit: cover;
      transition: transform 0.4s ease;
    }

    .gallery-item:hover {
      transform: scale(1.03);
      border-color: var(--primary);
      box-shadow: 0 15px 40px rgba(0, 217, 255, 0.3);
    }

    .gallery-item:hover img {
      transform: scale(1.1);
    }

    /* Lightbox */
    .lightbox {
      display: none;
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      height: 100%;
      background: rgba(0, 0, 0, 0.95);
      z-index: 9999;
      align-items: center;
      justify-content: center;
    }

    .lightbox.active {
      display: flex;
    }

    .lightbox img {
      max-width: 90%;
      max-height: 90%;
      border-radius: 10px;
      box-shadow: 0 0 50px rgba(0, 217, 255, 0.5);
    }

    .lightbox-close, .lightbox-nav {
      position: absolute;
      color: var(--primary);
      font-size: 3rem;
      cursor: pointer;
      transition: all 0.3s ease;
      user-select: none;
    }

    .lightbox-close {
      top: 2rem;
      right: 3rem;
    }

    .lightbox-nav {
      font-size: 4rem;
      top: 50%;
      transform: translateY(-50%);
    }

    .lightbox-prev { left: 2rem; }
    .lightbox-next { right: 2rem; }

    .lightbox-close:hover, .lightbox-nav:hover {
      color: var(--secondary);
      text-shadow: 0 0 20px var(--secondary);
    }

    /* Footer */
    footer {
      background: var(--dark-alt);
      padding: 3rem 2rem;
      text-align: center;
      border-top: 1px solid rgba(0, 217, 255, 0.1);
      position: relative;
      z-index: 1;
    }

    footer p {
      color: var(--text-muted);
      font-family: 'DM Mono', monospace;
    }

    /* Stats Banner */
    .stats-banner {
      background: var(--gradient-3);
      padding: 3rem 2rem;
      margin: 4rem 0;
      border-radius: 20px;
      display: grid;
      grid-template-columns: repeat(auto-fit, minmax(200px, 1fr));
      gap: 2rem;
      box-shadow: 0 15px 50px rgba(0, 217, 255, 0.3);
    }

    .stat-item {
      text-align: center;
      color: white;
    }

    .stat-number {
      font-family: 'Audiowide', cursive;
      font-size: 3rem;
      font-weight: 700;
      display: block;
      margin-bottom: 0.5rem;
    }

    .stat-label {
      font-size: 1rem;
      opacity: 0.9;
      font-family: 'DM Mono', monospace;
      text-transform: uppercase;
      letter-spacing: 1px;
    }

    /* Responsive */
    @media (max-width: 968px) {
      .nav-links {
        position: fixed;
        top: 0;
        right: -100%;
        width: 300px;
        height: 100vh;
        background: var(--dark-card);
        flex-direction: column;
        justify-content: center;
        gap: 2rem;
        transition: right 0.4s ease;
        border-left: 1px solid rgba(0, 217, 255, 0.2);
      }

      .nav-links.active {
        right: 0;
      }

      .menu-toggle {
        display: flex;
      }

      .menu-toggle.active span:nth-child(1) {
        transform: rotate(45deg) translate(8px, 8px);
      }

      .menu-toggle.active span:nth-child(2) {
        opacity: 0;
      }

      .menu-toggle.active span:nth-child(3) {
        transform: rotate(-45deg) translate(7px, -7px);
      }

      .schedule-item {
        grid-template-columns: 1fr;
        gap: 1rem;
      }

      .schedule-actions {
        justify-content: flex-start;
      }

      .people-grid {
        grid-template-columns: 1fr;
      }

      .hero-meta {
        flex-direction: column;
        gap: 1rem;
      }
    }

    @media (max-width: 640px) {
      .person-header {
        flex-direction: column;
        align-items: center;
        text-align: center;
      }

      .gallery-grid {
        grid-template-columns: 1fr;
      }

      .stats-banner {
        grid-template-columns: 1fr;
      }
    }

    /* Scroll animations */
    .fade-in {
      opacity: 0;
      transform: translateY(30px);
      transition: all 0.6s ease;
    }

    .fade-in.visible {
      opacity: 1;
      transform: translateY(0);
    }
  </style>
</head>
<body>
  <!-- Animated Background -->
  <canvas class="bg-canvas"></canvas>

  <!-- Navigation -->
  <nav id="nav">
    <div class="nav-container">
      <div class="logo">
        <div class="logo-icon">V+L</div>
        <span>VL REASONING</span>
      </div>
      <ul class="nav-links" id="navLinks">
        <li><a href="#intro">Introduction</a></li>
        <li><a href="#schedule">Schedule</a></li>
        <li><a href="#organizers">Organizers</a></li>
        <li><a href="#speakers">Speakers</a></li>
        <li><a href="#gallery">Gallery</a></li>
        <li><a href="https://drive.google.com/file/d/1t5j2LousMZtKUeUgO0iARJb4JFFSSj5F/view?usp=sharing" target="_blank" class="zoom-btn">
          <i class="fas fa-video"></i> Join Zoom
        </a></li>
      </ul>
      <div class="menu-toggle" id="menuToggle">
        <span></span>
        <span></span>
        <span></span>
      </div>
    </div>
  </nav>

  <!-- Hero Section -->
  <section class="hero">
    <div class="hero-content">
      <div class="subtitle">ICCV 2025 Tutorial</div>
      <h1>Towards Comprehensive Reasoning in Vision-Language Models</h1>
      <div class="hero-meta">
        <div class="hero-meta-item">
          <i class="fa fa-calendar-days"></i>
          <span>10/19/2025 8:30-12:00 (GMT-10)</span>
        </div>
        <div class="hero-meta-item">
          <i class="fa fa-location-dot"></i>
          <span>Room 318A, Hawai'i Convention Center</span>
        </div>
      </div>
    </div>
  </section>

  <!-- Introduction -->
  <section id="intro">
    <div class="container">
      <h2 class="section-title fade-in">Introduction</h2>
      <p class="section-subtitle fade-in">Bridging Vision and Language Through Advanced Reasoning</p>
      
      <div class="intro-text fade-in">
        <p>Vision-Language Models (VLMs) have achieved remarkable progress in image captioning and visual question answering, yet developing genuine reasoning capabilities remains an open challenge. Unlike recent breakthroughs in reasoning-focused LLMs, many VLMs still rely primarily on pattern recognition and struggle with compositional logic.</p>
        
        <p style="margin-top: 1.5rem;">This tutorial provides a comprehensive overview of reasoning capabilities in VLMs, focusing on the transition from basic perception to complex inference. We will explore reasoning-oriented prompting and training techniques in multimodal contexts, reasoning-focused benchmarks, and architectural innovations for visual-textual fusion.</p>

        <p style="margin-top: 1.5rem;">Through lectures and hands-on demonstrations, participants will gain insights into current capabilities, persistent challenges in compositional generalization and explainability, and practical guidance for implementing reasoning mechanisms. This tutorial uniquely bridges advances in LLM reasoning with the visual domain, addressing the distinct challenges of spatial information processing and providing a roadmap toward more cognitively capable vision-language systems.</p>
      </div>

      <div class="stats-banner fade-in">
        <div class="stat-item">
          <span class="stat-number">931</span>
          <span class="stat-label">Page Views</span>
        </div>
        <div class="stat-item">
          <span class="stat-number">218</span>
          <span class="stat-label">Bookmarks</span>
        </div>
        <div class="stat-item">
          <span class="stat-number">158</span>
          <span class="stat-label">Zoom Attendees</span>
        </div>
        <div class="stat-item">
          <span class="stat-number">200+</span>
          <span class="stat-label">In-Person</span>
        </div>
      </div>
    </div>
  </section>

  <!-- Schedule -->
  <section id="schedule">
    <div class="container">
      <h2 class="section-title fade-in">Tutorial Schedule</h2>
      <p class="section-subtitle fade-in">A Full Morning of Cutting-Edge Research</p>
      
      <div class="schedule-container fade-in">
        <div class="schedule-item">
          <div class="schedule-time">8:30 - 8:35</div>
          <div class="schedule-content">
            <h3>Opening Remark: Motivation and Overview</h3>
            <div class="schedule-speaker">
              <a href="https://vanoracai.github.io/" target="_blank">Yujun Cai</a>
            </div>
          </div>
          <div class="schedule-actions">
            <button class="btn-abstract" onclick="toggleAbstract(this)">Abstract</button>
            <a href="assets/slides/iccv_tutorial_open_remark_yujun.pdf" class="btn-slides" target="_blank">Slides</a>
          </div>
          <div class="abstract-content">
            Welcome to our comprehensive tutorial on reasoning in vision-language models. This opening session will set the stage by discussing the motivation behind developing reasoning capabilities in VLMs and provide an overview of the day's agenda, highlighting key challenges and opportunities in the field.
          </div>
        </div>

        <div class="schedule-item">
          <div class="schedule-time">8:35 - 9:10</div>
          <div class="schedule-content">
            <h3>Native Multimodal Models: Architecture, Post-Training, and Evaluation</h3>
            <div class="schedule-speaker">
              <a href="https://liuziwei7.github.io/" target="_blank">Ziwei Liu</a>
            </div>
          </div>
          <div class="schedule-actions">
            <button class="btn-abstract" onclick="toggleAbstract(this)">Abstract</button>
            <a href="assets/slides/ziweiliu.pdf" class="btn-slides" target="_blank">Slides</a>
          </div>
          <div class="abstract-content">
            As we strive toward more capable and general-purpose AI systems, the integration of vision and language in multimodal models (VLMs) has become a pivotal area of research. This talk explores three complementary advances that collectively push the boundaries of what native LMMs can achieve, spanning their architecture, post-training, and evaluation.
          </div>
        </div>

        <div class="schedule-item">
          <div class="schedule-time">9:10 - 9:35</div>
          <div class="schedule-content">
            <h3>Video-TT Challenge: Towards Advanced Video Reasoning and Understanding</h3>
            <div class="schedule-speaker">
              <a href="https://sites.google.com/view/video-tt-challenge" target="_blank">Yuhao Dong, Yuanhan Zhang, Ziwei Liu, and Teams</a>
            </div>
          </div>
          <div class="schedule-actions">
            <button class="btn-abstract" onclick="toggleAbstract(this)">Abstract</button>
            <a href="assets/slides/video_tt_challenge.pdf" class="btn-slides" target="_blank">Slides</a>
          </div>
          <div class="abstract-content">
            Join us for the exciting conclusion of the Video-TT Challenge, where we will announce results and recognize outstanding achievements in video reasoning and understanding. This session will showcase innovative approaches from participating teams and highlight breakthrough methodologies that advance the state-of-the-art in video analysis and temporal reasoning.
          </div>
        </div>

        <div class="schedule-item">
          <div class="schedule-time">9:35 - 10:10</div>
          <div class="schedule-content">
            <h3>Reasoning in Multimodal GUI Agents: An Exploration-Driven Perspective</h3>
            <div class="schedule-speaker">
              <a href="https://icoz69.github.io/" target="_blank">Chi Zhang</a>
            </div>
          </div>
          <div class="schedule-actions">
            <button class="btn-abstract" onclick="toggleAbstract(this)">Abstract</button>
            <a href="assets/slides/chizhang.pdf" class="btn-slides" target="_blank">Slides</a>
          </div>
          <div class="abstract-content">
            This talk explores how multimodal GUI agents can develop sophisticated reasoning capabilities through exploration-driven learning. We will discuss novel approaches to understanding user interfaces, spatial relationships, and interaction patterns, highlighting how agents can learn to navigate complex digital environments through systematic exploration and reasoning.
          </div>
        </div>

        <div class="schedule-item">
          <div class="schedule-time">10:10 - 10:45</div>
          <div class="schedule-content">
            <h3>Mathematical Reasoning in Visual Contexts</h3>
            <div class="schedule-speaker">
              <a href="https://web.cs.ucla.edu/~kwchang/" target="_blank">Kai-Wei Chang</a>
            </div>
          </div>
          <div class="schedule-actions">
            <button class="btn-abstract" onclick="toggleAbstract(this)">Abstract</button>
            <a href="assets/slides/kai-wei-chang-math-reasoning.pdf" class="btn-slides" target="_blank">Slides</a>
          </div>
          <div class="abstract-content">
            Mathematical reasoning presents unique challenges when combined with visual information. This talk will explore how vision-language models can be enhanced to solve mathematical problems that require understanding geometric relationships, interpreting charts and graphs, and reasoning about spatial configurations in mathematical contexts.
          </div>
        </div>

        <div class="schedule-item">
          <div class="schedule-time">10:45 - 11:20</div>
          <div class="schedule-content">
            <h3>Chain-of-Look Visual Reasoning</h3>
            <div class="schedule-speaker">
              <a href="https://cse.buffalo.edu/~jsyuan/" target="_blank">Junsong Yuan</a>
            </div>
          </div>
          <div class="schedule-actions">
            <button class="btn-abstract" onclick="toggleAbstract(this)">Abstract</button>
            <a href="assets/slides/junsong.pdf" class="btn-slides" target="_blank">Slides</a>
          </div>
          <div class="abstract-content">
            While multi-modal foundation models excel at describing images and answering simple questions, they still struggle at tasks requiring deliberate, step-by-step visual reasoning. We introduce Chain-of-Look, a new visual reasoning paradigm that addresses this weakness by modeling sequential visual understanding, leading to more accurate, robust, and explainable reasoning in challenging scenes.
          </div>
        </div>

        <div class="schedule-item">
          <div class="schedule-time">11:20 - 11:55</div>
          <div class="schedule-content">
            <h3>Grounding Anything in Images and Videos for Comprehensive Reasoning</h3>
            <div class="schedule-speaker">
              <a href="https://faculty.ucmerced.edu/mhyang/" target="_blank">Ming-Hsuan Yang</a>
            </div>
          </div>
          <div class="schedule-actions">
            <button class="btn-abstract" onclick="toggleAbstract(this)">Abstract</button>
            <a href="assets/slides/minghsuan.pdf" class="btn-slides" target="_blank">Slides</a>
          </div>
          <div class="abstract-content">
            Understanding and reasoning about the visual world requires more than recognizing objects—it demands grounding language, actions, and abstract concepts in images and videos in a unified and interpretable way. This talk explores the emerging paradigm of comprehensive grounding, which bridges perception and reasoning by linking every visual element to structured representations.
          </div>
        </div>

        <div class="schedule-item">
          <div class="schedule-time">11:55 - 12:00</div>
          <div class="schedule-content">
            <h3>Closing Remark</h3>
            <div class="schedule-speaker">
              <a href="https://wangywust.github.io/" target="_blank">Yiwei Wang</a>
            </div>
          </div>
          <div class="schedule-actions">
            <button class="btn-abstract" onclick="toggleAbstract(this)">Abstract</button>
            <a href="assets/slides/closing-remark.pdf" class="btn-slides" target="_blank">Slides</a>
          </div>
          <div class="abstract-content">
            Our tutorial concludes with a synthesis of key insights, future research directions, and practical takeaways. This closing session will summarize the day's discussions, highlight emerging trends in vision-language reasoning, and provide guidance for researchers and practitioners looking to advance this exciting field.
          </div>
        </div>
      </div>
    </div>
  </section>

  <!-- Organizers -->
  <section id="organizers">
    <div class="container">
      <h2 class="section-title fade-in">Organizers</h2>
      <p class="section-subtitle fade-in">Leading Experts in Vision-Language Research</p>
      
      <div class="people-grid">
        <div class="person-card fade-in">
          <div class="person-header">
            <img src="assets/images/avatars/yujuncai.png" alt="Yujun Cai" class="person-avatar">
            <div class="person-info">
              <h3>Yujun Cai</h3>
              <p class="person-affiliation">Lecturer at University of Queensland</p>
            </div>
          </div>
          <p class="person-bio">Dr. Yujun Cai is a Lecturer (Assistant Professor) in the University of Queensland, Australia. Previously, she was a Research Scientist at Meta Reality Labs in Seattle. She obtained her Ph.D. degree from Nanyang Technological University in 2021. Her research interests lie in multi-modal understanding and trust-worthy large models.</p>
          <div class="person-links">
            <a href="https://vanoracai.github.io/" target="_blank"><i class="fa fa-house"></i></a>
            <a href="https://scholar.google.com/citations?user=TE7lbQwAAAAJ" target="_blank"><i class="fas fa-graduation-cap"></i></a>
            <a href="https://github.com/vanoracai" target="_blank"><i class="fab fa-github"></i></a>
          </div>
        </div>

        <div class="person-card fade-in">
          <div class="person-header">
            <img src="assets/images/avatars/junliu.jpg" alt="Jun Liu" class="person-avatar">
            <div class="person-info">
              <h3>Jun Liu</h3>
              <p class="person-affiliation">Professor at Lancaster University</p>
            </div>
          </div>
          <p class="person-bio">Dr. Jun Liu is Professor and Chair in Digital Health at the School of Computing and Communications, Lancaster University. He earned his PhD from Nanyang Technological University in 2019, subsequently serving as faculty at Singapore University of Technology and Design from 2019 to 2024.</p>
          <div class="person-links">
            <a href="https://wp.lancs.ac.uk/vl/" target="_blank"><i class="fa fa-house"></i></a>
            <a href="https://scholar.google.com/citations?user=Q5Ild8UAAAAJ" target="_blank"><i class="fas fa-graduation-cap"></i></a>
            <a href="https://github.com/junliu" target="_blank"><i class="fab fa-github"></i></a>
          </div>
        </div>

        <div class="person-card fade-in">
          <div class="person-header">
            <img src="assets/images/avatars/yiweiwang.jpg" alt="Yiwei Wang" class="person-avatar">
            <div class="person-info">
              <h3>Yiwei Wang</h3>
              <p class="person-affiliation">Assistant Professor at UC Merced</p>
            </div>
          </div>
          <p class="person-bio">Dr. Yiwei Wang was an Applied Scientist in Amazon (Seattle) in 2023 and a Postdoc in UCLA NLP Group in 2024. He obtained his Ph.D. degree from National University of Singapore in 2023. Currently, he leads the UC Merced NLP Lab, where his team explores cutting-edge approaches to diffusion llms, reasoning multi-modal llms, and their applications.</p>
          <div class="person-links">
            <a href="https://wangywust.github.io/" target="_blank"><i class="fa fa-house"></i></a>
            <a href="https://scholar.google.com/citations?user=Sh9QvBkAAAAJ" target="_blank"><i class="fas fa-graduation-cap"></i></a>
            <a href="https://github.com/wangywust" target="_blank"><i class="fab fa-github"></i></a>
          </div>
        </div>

        <div class="person-card fade-in">
          <div class="person-header">
            <img src="assets/images/avatars/minghsuanyang.jpg" alt="Ming-Hsuan Yang" class="person-avatar">
            <div class="person-info">
              <h3>Ming-Hsuan Yang</h3>
              <p class="person-affiliation">Professor at UC Merced</p>
            </div>
          </div>
          <p class="person-bio">Ming-Hsuan Yang is a Professor in Electrical Engineering and Computer Science at University of California, Merced. He is a Fellow of the IEEE, ACM and AAAI.</p>
          <div class="person-links">
            <a href="https://faculty.ucmerced.edu/mhyang/" target="_blank"><i class="fa fa-house"></i></a>
            <a href="https://scholar.google.com/citations?user=p9-ohHsAAAAJ" target="_blank"><i class="fas fa-graduation-cap"></i></a>
            <a href="https://github.com/mhyang" target="_blank"><i class="fab fa-github"></i></a>
          </div>
        </div>
      </div>
    </div>
  </section>

  <!-- Speakers -->
  <section id="speakers">
    <div class="container">
      <h2 class="section-title fade-in">Invited Speakers</h2>
      <p class="section-subtitle fade-in">World-Class Researchers Sharing Their Insights</p>
      
      <div class="people-grid">
        <div class="person-card fade-in">
          <div class="person-header">
            <img src="assets/images/avatars/kaiweichang.jpg" alt="Kai-Wei Chang" class="person-avatar">
            <div class="person-info">
              <h3>Kai-Wei Chang</h3>
              <p class="person-affiliation">Associate Professor at UCLA</p>
            </div>
          </div>
          <p class="person-bio">Dr. Kai-Wei Chang is an Associate Professor at UCLA specializing in natural language processing, machine learning, and multimodal reasoning. His research particularly focuses on mathematical reasoning, structured prediction, and developing robust AI systems that can handle complex reasoning tasks.</p>
          <div class="person-links">
            <a href="https://web.cs.ucla.edu/~kwchang/" target="_blank"><i class="fa fa-house"></i></a>
            <a href="https://scholar.google.com/citations?user=fqDBtzYAAAAJ" target="_blank"><i class="fas fa-graduation-cap"></i></a>
            <a href="https://github.com/kwchang" target="_blank"><i class="fab fa-github"></i></a>
          </div>
        </div>

        <div class="person-card fade-in">
          <div class="person-header">
            <img src="assets/images/avatars/junsongyuan.jpg" alt="Junsong Yuan" class="person-avatar">
            <div class="person-info">
              <h3>Junsong Yuan</h3>
              <p class="person-affiliation">Professor at University at Buffalo</p>
            </div>
          </div>
          <p class="person-bio">Dr. Junsong Yuan is a Professor at University at Buffalo, specializing in computer vision, pattern recognition, and multimedia analysis. His research encompasses video understanding, human activity recognition, and multimodal learning with applications in surveillance, healthcare, and autonomous systems.</p>
          <div class="person-links">
            <a href="https://cse.buffalo.edu/~jsyuan/" target="_blank"><i class="fa fa-house"></i></a>
            <a href="https://scholar.google.com/citations?user=fJ7seq0AAAAJ" target="_blank"><i class="fas fa-graduation-cap"></i></a>
            <a href="https://github.com/jsyuan" target="_blank"><i class="fab fa-github"></i></a>
          </div>
        </div>

        <div class="person-card fade-in">
          <div class="person-header">
            <img src="assets/images/avatars/ziweiliu.png" alt="Ziwei Liu" class="person-avatar">
            <div class="person-info">
              <h3>Ziwei Liu</h3>
              <p class="person-affiliation">Associate Professor at NTU</p>
            </div>
          </div>
          <p class="person-bio">Dr. Ziwei Liu is an Associate Professor at NTU and leads the LMMs-Lab initiative. His research focuses on large multimodal models, computer vision, and machine learning. He has made significant contributions to visual understanding, generative models, and multimodal intelligence systems.</p>
          <div class="person-links">
            <a href="https://liuziwei7.github.io/" target="_blank"><i class="fa fa-house"></i></a>
            <a href="https://scholar.google.com/citations?user=lc45xlcAAAAJ" target="_blank"><i class="fas fa-graduation-cap"></i></a>
            <a href="https://github.com/liuziwei7" target="_blank"><i class="fab fa-github"></i></a>
          </div>
        </div>

        <div class="person-card fade-in">
          <div class="person-header">
            <img src="assets/images/avatars/chizhang.jpg" alt="Chi Zhang" class="person-avatar">
            <div class="person-info">
              <h3>Chi Zhang</h3>
              <p class="person-affiliation">Assistant Professor at Westlake University</p>
            </div>
          </div>
          <p class="person-bio">Dr. Chi Zhang is an Assistant Professor at Westlake University, focusing on multimodal AI and embodied intelligence. His research spans GUI automation, visual reasoning, and human-computer interaction, with particular expertise in developing intelligent agents that can understand and interact with digital interfaces.</p>
          <div class="person-links">
            <a href="https://icoz69.github.io/" target="_blank"><i class="fa fa-house"></i></a>
            <a href="https://scholar.google.com/citations?user=J4s398EAAAAJ" target="_blank"><i class="fas fa-graduation-cap"></i></a>
            <a href="https://github.com/icoz69" target="_blank"><i class="fab fa-github"></i></a>
          </div>
        </div>

        <div class="person-card fade-in">
          <div class="person-header">
            <img src="assets/images/avatars/yuhaodong.jpg" alt="Yuhao Dong" class="person-avatar">
            <div class="person-info">
              <h3>Yuhao Dong</h3>
              <p class="person-affiliation">Ph.D. Student at NTU</p>
            </div>
          </div>
          <p class="person-bio">Yuhao Dong currently working on the topic of multimodal learning and his long-term goal is to build general foundation models. Recently he is focusing on vision-language model and unified visual models.</p>
          <div class="person-links">
            <a href="https://github.com/dongyh20" target="_blank"><i class="fa fa-house"></i></a>
            <a href="https://scholar.google.com/citations?user=kMui170AAAAJ" target="_blank"><i class="fas fa-graduation-cap"></i></a>
            <a href="https://github.com/dongyh20" target="_blank"><i class="fab fa-github"></i></a>
          </div>
        </div>

        <div class="person-card fade-in">
          <div class="person-header">
            <img src="assets/images/avatars/yuanhanzhang.jpg" alt="Yuanhan Zhang" class="person-avatar">
            <div class="person-info">
              <h3>Yuanhan Zhang</h3>
              <p class="person-affiliation">Ph.D. Student at NTU</p>
            </div>
          </div>
          <p class="person-bio">Yuanhan Zhang's research interests lie in computer vision and deep learning. His work focuses on adapting foundation models—from vision to multimodal—for real-world applications, including benchmarking model performance and adapting models through parameter-efficient tuning, in-context learning, and instruction tuning.</p>
          <div class="person-links">
            <a href="https://zhangyuanhan-ai.github.io/" target="_blank"><i class="fa fa-house"></i></a>
            <a href="https://scholar.google.com/citations?user=g6grFy0AAAAJ" target="_blank"><i class="fas fa-graduation-cap"></i></a>
            <a href="https://github.com/zhangyuanhan" target="_blank"><i class="fab fa-github"></i></a>
          </div>
        </div>
      </div>
    </div>
  </section>

  <!-- Gallery -->
  <section id="gallery">
    <div class="container">
      <h2 class="section-title fade-in">Gallery</h2>
      <p class="section-subtitle fade-in">Moments from the Tutorial</p>
      
      <div class="gallery-grid" id="galleryGrid"></div>
    </div>
  </section>

  <!-- Lightbox -->
  <div class="lightbox" id="lightbox">
    <span class="lightbox-close" onclick="closeLightbox()">&times;</span>
    <span class="lightbox-nav lightbox-prev" onclick="changeImage(-1)">&#10094;</span>
    <img src="" alt="Gallery Image" id="lightboxImg">
    <span class="lightbox-nav lightbox-next" onclick="changeImage(1)">&#10095;</span>
  </div>

  <!-- Footer -->
  <footer>
    <div class="container">
      <p>&copy; 2025 Towards Comprehensive Reasoning in Vision-Language Models | ICCV Tutorial</p>
    </div>
  </footer>

  <script>
    // Animated background
    const canvas = document.querySelector('.bg-canvas');
    const ctx = canvas.getContext('2d');
    canvas.width = window.innerWidth;
    canvas.height = window.innerHeight;

    const particles = [];
    const particleCount = 80;

    class Particle {
      constructor() {
        this.x = Math.random() * canvas.width;
        this.y = Math.random() * canvas.height;
        this.vx = (Math.random() - 0.5) * 0.5;
        this.vy = (Math.random() - 0.5) * 0.5;
        this.radius = Math.random() * 2 + 1;
      }

      update() {
        this.x += this.vx;
        this.y += this.vy;

        if (this.x < 0 || this.x > canvas.width) this.vx *= -1;
        if (this.y < 0 || this.y > canvas.height) this.vy *= -1;
      }

      draw() {
        ctx.beginPath();
        ctx.arc(this.x, this.y, this.radius, 0, Math.PI * 2);
        ctx.fillStyle = 'rgba(0, 217, 255, 0.5)';
        ctx.fill();
      }
    }

    for (let i = 0; i < particleCount; i++) {
      particles.push(new Particle());
    }

    function animate() {
      ctx.clearRect(0, 0, canvas.width, canvas.height);
      
      particles.forEach(particle => {
        particle.update();
        particle.draw();
      });

      // Draw connections
      particles.forEach((p1, i) => {
        particles.slice(i + 1).forEach(p2 => {
          const dx = p1.x - p2.x;
          const dy = p1.y - p2.y;
          const distance = Math.sqrt(dx * dx + dy * dy);

          if (distance < 150) {
            ctx.beginPath();
            ctx.moveTo(p1.x, p1.y);
            ctx.lineTo(p2.x, p2.y);
            ctx.strokeStyle = `rgba(0, 217, 255, ${0.2 * (1 - distance / 150)})`;
            ctx.lineWidth = 1;
            ctx.stroke();
          }
        });
      });

      requestAnimationFrame(animate);
    }

    animate();

    window.addEventListener('resize', () => {
      canvas.width = window.innerWidth;
      canvas.height = window.innerHeight;
    });

    // Navigation scroll effect
    const nav = document.getElementById('nav');
    window.addEventListener('scroll', () => {
      if (window.scrollY > 50) {
        nav.classList.add('scrolled');
      } else {
        nav.classList.remove('scrolled');
      }
    });

    // Mobile menu
    const menuToggle = document.getElementById('menuToggle');
    const navLinks = document.getElementById('navLinks');

    menuToggle.addEventListener('click', () => {
      menuToggle.classList.toggle('active');
      navLinks.classList.toggle('active');
    });

    // Close menu when clicking a link
    document.querySelectorAll('.nav-links a').forEach(link => {
      link.addEventListener('click', () => {
        menuToggle.classList.remove('active');
        navLinks.classList.remove('active');
      });
    });

    // Smooth scroll
    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
      anchor.addEventListener('click', function (e) {
        e.preventDefault();
        const target = document.querySelector(this.getAttribute('href'));
        if (target) {
          window.scrollTo({
            top: target.offsetTop - 80,
            behavior: 'smooth'
          });
        }
      });
    });

    // Abstract toggle
    function toggleAbstract(btn) {
      const item = btn.closest('.schedule-item');
      const abstract = item.querySelector('.abstract-content');
      abstract.classList.toggle('active');
      btn.textContent = abstract.classList.contains('active') ? 'Hide' : 'Abstract';
    }

    // Scroll animations
    const observerOptions = {
      threshold: 0.1,
      rootMargin: '0px 0px -50px 0px'
    };

    const observer = new IntersectionObserver((entries) => {
      entries.forEach(entry => {
        if (entry.isIntersecting) {
          entry.target.classList.add('visible');
        }
      });
    }, observerOptions);

    document.querySelectorAll('.fade-in').forEach(el => observer.observe(el));

    // Gallery
    const galleryFolder = 'assets/gallery/';
    const photoFiles = Array.from({length: 31}, (_, i) => `photo${i + 1}.jpg`);
    const galleryGrid = document.getElementById('galleryGrid');
    let currentImageIndex = 0;

    photoFiles.forEach((photo, index) => {
      const item = document.createElement('div');
      item.className = 'gallery-item fade-in';
      item.innerHTML = `<img src="${galleryFolder}${photo}" alt="Gallery Photo ${index + 1}">`;
      item.addEventListener('click', () => openLightbox(index));
      galleryGrid.appendChild(item);
      observer.observe(item);
    });

    function openLightbox(index) {
      currentImageIndex = index;
      document.getElementById('lightboxImg').src = galleryFolder + photoFiles[index];
      document.getElementById('lightbox').classList.add('active');
      document.body.style.overflow = 'hidden';
    }

    function closeLightbox() {
      document.getElementById('lightbox').classList.remove('active');
      document.body.style.overflow = '';
    }

    function changeImage(direction) {
      currentImageIndex = (currentImageIndex + direction + photoFiles.length) % photoFiles.length;
      document.getElementById('lightboxImg').src = galleryFolder + photoFiles[currentImageIndex];
    }

    // Keyboard navigation
    document.addEventListener('keydown', (e) => {
      const lightbox = document.getElementById('lightbox');
      if (!lightbox.classList.contains('active')) return;
      if (e.key === 'Escape') closeLightbox();
      if (e.key === 'ArrowRight') changeImage(1);
      if (e.key === 'ArrowLeft') changeImage(-1);
    });

    // Close lightbox on background click
    document.getElementById('lightbox').addEventListener('click', (e) => {
      if (e.target.id === 'lightbox') closeLightbox();
    });
  </script>
</body>
</html>