<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
  <title>Towards Comprehensive Reasoning in Vision-Language Models | ICCV 2025</title>
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css" />
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Playfair+Display:wght@400;600;700;900&family=Source+Sans+3:wght@300;400;600;700&family=Fira+Code:wght@400;500&display=swap" rel="stylesheet">
  <style>
    :root {
      --primary: #1a4d7a;
      --primary-light: #2968a3;
      --accent: #c17d11;
      --accent-light: #d4941d;
      --dark: #1a1a1a;
      --dark-surface: #2a2a2a;
      --light-bg: #f8f8f8;
      --text-primary: #2c2c2c;
      --text-secondary: #666666;
      --border: #d4d4d4;
      --shadow: rgba(0, 0, 0, 0.08);
    }

    * {
      margin: 0;
      padding: 0;
      box-sizing: border-box;
    }

    body {
      font-family: 'Source Sans 3', sans-serif;
      color: var(--text-primary);
      line-height: 1.7;
      background: #ffffff;
    }

    /* Typography */
    h1, h2, h3, h4 {
      font-family: 'Playfair Display', serif;
      font-weight: 700;
      line-height: 1.2;
    }

    /* Navigation */
    nav {
      position: fixed;
      top: 0;
      width: 100%;
      background: rgba(255, 255, 255, 0.98);
      backdrop-filter: blur(10px);
      border-bottom: 1px solid var(--border);
      z-index: 1000;
      transition: all 0.3s ease;
    }

    nav.scrolled {
      box-shadow: 0 2px 20px var(--shadow);
    }

    .nav-container {
      max-width: 1600px;
      margin: 0 auto;
      padding: 0.9rem 2rem;
      display: flex;
      justify-content: space-between;
      align-items: center;
    }

    .logo {
      display: flex;
      align-items: center;
      gap: 0.7rem;
      font-family: 'Playfair Display', serif;
      font-size: 1.1rem;
      font-weight: 700;
      color: var(--primary);
    }

    .logo-mark {
      width: 40px;
      height: 40px;
      background: linear-gradient(135deg, var(--primary) 0%, var(--primary-light) 100%);
      border-radius: 4px;
      display: flex;
      align-items: center;
      justify-content: center;
      color: white;
      font-weight: 900;
      font-size: 1rem;
      letter-spacing: -0.5px;
    }
    
    .logo-text {
      display: flex;
      flex-direction: column;
      line-height: 1.2;
    }
    
    .logo-title {
      font-size: 1.1rem;
      font-weight: 700;
    }
    
    .logo-subtitle {
      font-size: 0.75rem;
      font-weight: 500;
      color: var(--text-secondary);
      text-transform: uppercase;
      letter-spacing: 0.5px;
    }

    .nav-links {
      display: flex;
      gap: 2.5rem;
      list-style: none;
      align-items: center;
    }

    .nav-links a {
      color: var(--text-primary);
      text-decoration: none;
      font-weight: 500;
      font-size: 1rem;
      position: relative;
      transition: color 0.3s ease;
      letter-spacing: 0.3px;
    }

    .nav-links a::after {
      content: '';
      position: absolute;
      bottom: -5px;
      left: 0;
      width: 0;
      height: 2px;
      background: var(--primary);
      transition: width 0.3s ease;
    }

    .nav-links a:hover {
      color: var(--primary);
    }

    .nav-links a:hover::after {
      width: 100%;
    }

    .cta-button {
      background: var(--primary);
      color: white;
      padding: 0.75rem 1.8rem;
      border-radius: 4px;
      text-decoration: none;
      font-weight: 600;
      transition: all 0.3s ease;
      font-size: 0.95rem;
      letter-spacing: 0.3px;
    }

    .cta-button:hover {
      background: var(--primary-light);
      transform: translateY(-2px);
      box-shadow: 0 4px 12px rgba(26, 77, 122, 0.3);
    }

    .cta-button i {
      margin-right: 0.5rem;
    }

    /* Mobile menu */
    .menu-toggle {
      display: none;
      flex-direction: column;
      gap: 5px;
      cursor: pointer;
    }

    .menu-toggle span {
      width: 28px;
      height: 3px;
      background: var(--primary);
      border-radius: 2px;
      transition: all 0.3s ease;
    }

    /* Hero Section */
    .hero {
      margin-top: 72px;
      background: linear-gradient(135deg, #f5f7fa 0%, #e8ecf1 100%);
      border-bottom: 1px solid var(--border);
      padding: 3.5rem 2rem 3rem;
      position: relative;
      overflow: hidden;
    }

    .hero::before {
      content: '';
      position: absolute;
      top: -50%;
      right: -10%;
      width: 600px;
      height: 600px;
      background: radial-gradient(circle, rgba(26, 77, 122, 0.08) 0%, transparent 70%);
      border-radius: 50%;
    }

    .hero-content {
      max-width: 1600px;
      margin: 0 auto;
      position: relative;
      z-index: 1;
    }

    .hero h1 {
      font-size: clamp(2.2rem, 4.5vw, 4rem);
      color: var(--text-primary);
      margin-bottom: 1.5rem;
      line-height: 1.15;
      max-width: 1200px;
    }

    .hero-meta {
      display: flex;
      gap: 2.5rem;
      margin-top: 1.8rem;
      flex-wrap: wrap;
    }

    .meta-item {
      display: flex;
      align-items: center;
      gap: 0.7rem;
      font-size: 1.05rem;
      color: var(--text-secondary);
      font-weight: 500;
    }

    .meta-item i {
      color: var(--accent);
      font-size: 1.2rem;
    }

    /* Section Styles */
    section {
      padding: 3.5rem 2rem;
    }

    section:nth-child(even) {
      background: var(--light-bg);
    }

    .container {
      max-width: 1600px;
      margin: 0 auto;
    }

    .section-header {
      margin-bottom: 2.5rem;
    }

    .section-title {
      font-size: clamp(2.2rem, 3.5vw, 3rem);
      color: var(--text-primary);
      margin-bottom: 0.6rem;
      position: relative;
      display: inline-block;
    }

    .section-title::after {
      content: '';
      position: absolute;
      bottom: -6px;
      left: 0;
      width: 60px;
      height: 3px;
      background: var(--accent);
    }

    .section-subtitle {
      font-size: 1.1rem;
      color: var(--text-secondary);
      margin-top: 1.2rem;
      font-weight: 400;
    }

    /* Introduction */
    .intro-content {
      max-width: 1400px;
      margin: 0 auto;
    }

    .intro-text {
      font-size: 1.1rem;
      line-height: 1.8;
      color: var(--text-primary);
      background: white;
      padding: 2rem 2.5rem;
      border-radius: 6px;
      border: 1px solid var(--border);
      box-shadow: 0 2px 12px var(--shadow);
    }

    .intro-text p + p {
      margin-top: 1.2rem;
    }

    /* Stats Section */
    .stats-grid {
      display: grid;
      grid-template-columns: repeat(auto-fit, minmax(200px, 1fr));
      gap: 1.5rem;
      margin-top: 2rem;
    }

    .stat-card {
      background: white;
      padding: 1.8rem 1.5rem;
      border-radius: 6px;
      border: 1px solid var(--border);
      text-align: center;
      transition: all 0.3s ease;
    }

    .stat-card:hover {
      transform: translateY(-4px);
      box-shadow: 0 6px 20px var(--shadow);
      border-color: var(--primary);
    }

    .stat-number {
      font-family: 'Playfair Display', serif;
      font-size: 3rem;
      font-weight: 700;
      color: var(--primary);
      display: block;
      margin-bottom: 0.4rem;
    }

    .stat-label {
      font-size: 0.95rem;
      color: var(--text-secondary);
      font-weight: 500;
      text-transform: uppercase;
      letter-spacing: 0.8px;
    }

    /* Schedule */
    .schedule-container {
      background: white;
      border-radius: 6px;
      border: 1px solid var(--border);
      overflow: hidden;
      box-shadow: 0 2px 12px var(--shadow);
    }

    .schedule-item {
      padding: 1.5rem;
      border-bottom: 1px solid var(--border);
      transition: background 0.3s ease;
      display: grid;
      grid-template-columns: 130px 1fr auto;
      gap: 2rem;
      align-items: start;
    }

    .schedule-item:hover {
      background: var(--light-bg);
    }

    .schedule-item:last-child {
      border-bottom: none;
    }

    .schedule-time {
      font-family: 'Fira Code', monospace;
      font-size: 0.95rem;
      color: var(--primary);
      font-weight: 600;
      padding-top: 0.15rem;
    }

    .schedule-content h3 {
      font-size: 1.2rem;
      margin-bottom: 0.5rem;
      color: var(--text-primary);
      font-weight: 600;
      line-height: 1.3;
    }

    .schedule-speaker {
      color: var(--text-secondary);
      font-size: 1rem;
      margin-top: 0.4rem;
    }

    .schedule-speaker a {
      color: var(--primary);
      text-decoration: none;
      font-weight: 500;
      transition: color 0.3s ease;
    }

    .schedule-speaker a:hover {
      color: var(--accent);
      text-decoration: underline;
    }

    .schedule-actions {
      display: flex;
      gap: 0.8rem;
      align-items: flex-start;
    }

    .btn {
      padding: 0.55rem 1.2rem;
      border-radius: 4px;
      text-decoration: none;
      font-size: 0.9rem;
      transition: all 0.3s ease;
      cursor: pointer;
      border: none;
      background: none;
      font-weight: 500;
      white-space: nowrap;
    }

    .btn-outline {
      border: 1.5px solid var(--primary);
      color: var(--primary);
      background: transparent;
    }

    .btn-outline:hover {
      background: var(--primary);
      color: white;
    }

    .btn-primary {
      background: var(--primary);
      color: white;
    }

    .btn-primary:hover {
      background: var(--primary-light);
      transform: translateY(-2px);
      box-shadow: 0 4px 12px rgba(26, 77, 122, 0.3);
    }

    .abstract-content {
      grid-column: 1 / -1;
      margin-top: 1.2rem;
      padding: 1.5rem;
      background: var(--light-bg);
      border-radius: 5px;
      border-left: 3px solid var(--accent);
      display: none;
      line-height: 1.7;
      color: var(--text-primary);
      font-size: 1rem;
    }

    .abstract-content.active {
      display: block;
      animation: slideDown 0.3s ease;
    }

    @keyframes slideDown {
      from {
        opacity: 0;
        transform: translateY(-10px);
      }
      to {
        opacity: 1;
        transform: translateY(0);
      }
    }

    /* People Cards */
    .people-grid {
      display: grid;
      grid-template-columns: repeat(auto-fill, minmax(360px, 1fr));
      gap: 2rem;
      margin-top: 2.5rem;
    }

    .person-card {
      background: white;
      border-radius: 6px;
      padding: 2rem;
      border: 1px solid var(--border);
      transition: all 0.3s ease;
      position: relative;
    }

    .person-card::before {
      content: '';
      position: absolute;
      top: 0;
      left: 0;
      width: 100%;
      height: 3px;
      background: linear-gradient(90deg, var(--primary) 0%, var(--accent) 100%);
      border-radius: 6px 6px 0 0;
      transform: scaleX(0);
      transform-origin: left;
      transition: transform 0.3s ease;
    }

    .person-card:hover {
      transform: translateY(-6px);
      box-shadow: 0 10px 28px var(--shadow);
    }

    .person-card:hover::before {
      transform: scaleX(1);
    }

    .person-header {
      display: flex;
      gap: 1.5rem;
      margin-bottom: 1.2rem;
    }

    .person-avatar {
      width: 100px;
      height: 100px;
      border-radius: 50%;
      border: 3px solid var(--primary);
      object-fit: cover;
      flex-shrink: 0;
    }

    .person-info h3 {
      font-size: 1.35rem;
      margin-bottom: 0.35rem;
      color: var(--text-primary);
    }

    .person-affiliation {
      color: var(--text-secondary);
      font-size: 0.95rem;
      line-height: 1.4;
    }

    .person-bio {
      color: var(--text-secondary);
      line-height: 1.7;
      font-size: 0.98rem;
    }

    .person-links {
      display: flex;
      gap: 0.9rem;
      margin-top: 1.2rem;
    }

    .person-links a {
      width: 36px;
      height: 36px;
      border-radius: 50%;
      border: 1.5px solid var(--primary);
      display: flex;
      align-items: center;
      justify-content: center;
      color: var(--primary);
      text-decoration: none;
      transition: all 0.3s ease;
      font-size: 1rem;
    }

    .person-links a:hover {
      background: var(--primary);
      color: white;
      transform: translateY(-3px);
      box-shadow: 0 4px 12px rgba(26, 77, 122, 0.3);
    }

    /* Gallery */
    .gallery-grid {
      display: grid;
      grid-template-columns: repeat(auto-fill, minmax(280px, 1fr));
      gap: 1.2rem;
      margin-top: 2.5rem;
    }

    .gallery-item {
      position: relative;
      border-radius: 5px;
      overflow: hidden;
      aspect-ratio: 4/3;
      cursor: pointer;
      transition: all 0.3s ease;
      border: 1px solid var(--border);
      background: var(--light-bg);
    }

    .gallery-item img {
      width: 100%;
      height: 100%;
      object-fit: cover;
      transition: transform 0.3s ease;
    }

    .gallery-item:hover {
      transform: translateY(-5px);
      box-shadow: 0 8px 24px var(--shadow);
      border-color: var(--primary);
    }

    .gallery-item:hover img {
      transform: scale(1.05);
    }

    /* Lightbox */
    .lightbox {
      display: none;
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      height: 100%;
      background: rgba(0, 0, 0, 0.92);
      z-index: 9999;
      align-items: center;
      justify-content: center;
    }

    .lightbox.active {
      display: flex;
    }

    .lightbox img {
      max-width: 90%;
      max-height: 90%;
      border-radius: 4px;
      box-shadow: 0 0 40px rgba(0, 0, 0, 0.5);
    }

    .lightbox-close, .lightbox-nav {
      position: absolute;
      color: white;
      font-size: 2.5rem;
      cursor: pointer;
      transition: all 0.3s ease;
      user-select: none;
      padding: 1rem;
    }

    .lightbox-close {
      top: 1.5rem;
      right: 2rem;
    }

    .lightbox-nav {
      font-size: 3rem;
      top: 50%;
      transform: translateY(-50%);
    }

    .lightbox-prev { left: 2rem; }
    .lightbox-next { right: 2rem; }

    .lightbox-close:hover, .lightbox-nav:hover {
      color: var(--accent);
    }

    /* Footer */
    footer {
      background: var(--dark);
      color: rgba(255, 255, 255, 0.8);
      padding: 2rem 2rem;
      text-align: center;
      border-top: 3px solid var(--primary);
    }

    footer p {
      font-size: 0.9rem;
      letter-spacing: 0.3px;
    }

    /* Animations */
    .fade-in {
      opacity: 0;
      transform: translateY(20px);
      transition: all 0.6s ease;
    }

    .fade-in.visible {
      opacity: 1;
      transform: translateY(0);
    }

    /* Responsive */
    @media (max-width: 968px) {
      .nav-links {
        position: fixed;
        top: 0;
        right: -100%;
        width: 320px;
        height: 100vh;
        background: white;
        flex-direction: column;
        justify-content: center;
        gap: 2rem;
        transition: right 0.4s ease;
        box-shadow: -4px 0 20px var(--shadow);
        padding: 2rem;
      }

      .nav-links.active {
        right: 0;
      }

      .menu-toggle {
        display: flex;
      }

      .menu-toggle.active span:nth-child(1) {
        transform: rotate(45deg) translate(8px, 8px);
      }

      .menu-toggle.active span:nth-child(2) {
        opacity: 0;
      }

      .menu-toggle.active span:nth-child(3) {
        transform: rotate(-45deg) translate(7px, -7px);
      }
      
      .logo-text {
        font-size: 0.95rem;
      }
      
      .logo-subtitle {
        font-size: 0.7rem;
      }

      .schedule-item {
        grid-template-columns: 1fr;
        gap: 1rem;
      }

      .schedule-actions {
        flex-direction: column;
      }

      .people-grid {
        grid-template-columns: 1fr;
      }

      .hero {
        padding: 2.5rem 1.5rem 2rem;
      }

      .hero-meta {
        flex-direction: column;
        gap: 0.8rem;
      }

      section {
        padding: 2.5rem 1.5rem;
      }
    }

    @media (max-width: 640px) {
      .person-header {
        flex-direction: column;
        align-items: center;
        text-align: center;
      }

      .gallery-grid {
        grid-template-columns: 1fr;
      }

      .stats-grid {
        grid-template-columns: 1fr;
      }

      .schedule-actions {
        width: 100%;
      }

      .btn {
        width: 100%;
        text-align: center;
      }
    }
  </style>
</head>
<body>
  <!-- Navigation -->
  <nav id="nav">
    <div class="nav-container">
      <div class="logo">
        <div class="logo-mark">VL</div>
        <div class="logo-text">
          <span class="logo-title">VL Reasoning</span>
          <span class="logo-subtitle">ICCV 2025</span>
        </div>
      </div>
      <ul class="nav-links" id="navLinks">
        <li><a href="#intro">Introduction</a></li>
        <li><a href="#schedule">Schedule</a></li>
        <li><a href="#organizers">Organizers</a></li>
        <li><a href="#speakers">Speakers</a></li>
        <li><a href="#gallery">Gallery</a></li>
        <li><a href="https://drive.google.com/file/d/1t5j2LousMZtKUeUgO0iARJb4JFFSSj5F/view?usp=sharing" target="_blank" class="cta-button">
          <i class="fas fa-video"></i> Join Zoom
        </a></li>
      </ul>
      <div class="menu-toggle" id="menuToggle">
        <span></span>
        <span></span>
        <span></span>
      </div>
    </div>
  </nav>

  <!-- Hero Section -->
  <section class="hero">
    <div class="hero-content">
      <h1>Towards Comprehensive Reasoning in Vision-Language Models</h1>
      <div class="hero-meta">
        <div class="meta-item">
          <i class="fa fa-tag"></i>
          <span>ICCV 2025 Tutorial</span>
        </div>
        <div class="meta-item">
          <i class="fa fa-calendar-days"></i>
          <span>October 19, 2025 • 8:30-12:00 (GMT-10)</span>
        </div>
        <div class="meta-item">
          <i class="fa fa-location-dot"></i>
          <span>Room 318A, Hawai'i Convention Center</span>
        </div>
      </div>
    </div>
  </section>

  <!-- Introduction -->
  <section id="intro">
    <div class="container">
      <div class="section-header fade-in">
        <h2 class="section-title">Introduction</h2>
        <p class="section-subtitle">Bridging Vision and Language Through Advanced Reasoning</p>
      </div>
      
      <div class="intro-content">
        <div class="intro-text fade-in">
          <p>Vision-Language Models (VLMs) have achieved remarkable progress in image captioning and visual question answering, yet developing genuine reasoning capabilities remains an open challenge. Unlike recent breakthroughs in reasoning-focused LLMs, many VLMs still rely primarily on pattern recognition and struggle with compositional logic.</p>
          
          <p>This tutorial provides a comprehensive overview of reasoning capabilities in VLMs, focusing on the transition from basic perception to complex inference. We will explore reasoning-oriented prompting and training techniques in multimodal contexts, reasoning-focused benchmarks, and architectural innovations for visual-textual fusion.</p>

          <p>Through lectures and hands-on demonstrations, participants will gain insights into current capabilities, persistent challenges in compositional generalization and explainability, and practical guidance for implementing reasoning mechanisms. This tutorial uniquely bridges advances in LLM reasoning with the visual domain, addressing the distinct challenges of spatial information processing and providing a roadmap toward more cognitively capable vision-language systems.</p>
        </div>

        <div class="stats-grid fade-in">
          <div class="stat-card">
            <span class="stat-number">931</span>
            <span class="stat-label">Page Views</span>
          </div>
          <div class="stat-card">
            <span class="stat-number">218</span>
            <span class="stat-label">Bookmarks</span>
          </div>
          <div class="stat-card">
            <span class="stat-number">158</span>
            <span class="stat-label">Zoom Attendees</span>
          </div>
          <div class="stat-card">
            <span class="stat-number">200+</span>
            <span class="stat-label">In-Person</span>
          </div>
        </div>
      </div>
    </div>
  </section>

  <!-- Schedule -->
  <section id="schedule">
    <div class="container">
      <div class="section-header fade-in">
        <h2 class="section-title">Tutorial Schedule</h2>
        <p class="section-subtitle">A Full Morning of Cutting-Edge Research</p>
      </div>
      
      <div class="schedule-container fade-in">
        <div class="schedule-item">
          <div class="schedule-time">8:30 - 8:35</div>
          <div class="schedule-content">
            <h3>Opening Remark: Motivation and Overview</h3>
            <div class="schedule-speaker">
              <a href="https://vanoracai.github.io/" target="_blank">Yujun Cai</a>
            </div>
          </div>
          <div class="schedule-actions">
            <button class="btn btn-outline" onclick="toggleAbstract(this)">Abstract</button>
            <a href="assets/slides/iccv_tutorial_open_remark_yujun.pdf" class="btn btn-primary" target="_blank">Slides</a>
          </div>
          <div class="abstract-content">
            <strong>Abstract:</strong> Welcome to our comprehensive tutorial on reasoning in vision-language models. This opening session will set the stage by discussing the motivation behind developing reasoning capabilities in VLMs and provide an overview of the day's agenda, highlighting key challenges and opportunities in the field.
          </div>
        </div>

        <div class="schedule-item">
          <div class="schedule-time">8:35 - 9:10</div>
          <div class="schedule-content">
            <h3>Native Multimodal Models: Architecture, Post-Training, and Evaluation</h3>
            <div class="schedule-speaker">
              <a href="https://liuziwei7.github.io/" target="_blank">Ziwei Liu</a>
            </div>
          </div>
          <div class="schedule-actions">
            <button class="btn btn-outline" onclick="toggleAbstract(this)">Abstract</button>
            <a href="assets/slides/ziweiliu.pdf" class="btn btn-primary" target="_blank">Slides</a>
          </div>
          <div class="abstract-content">
            <strong>Abstract:</strong> As we strive toward more capable and general-purpose AI systems, the integration of vision and language in multimodal models (VLMs) has become a pivotal area of research. This talk explores three complementary advances that collectively push the boundaries of what native LMMs can achieve, spanning their architecture, post-training, and evaluation.
          </div>
        </div>

        <div class="schedule-item">
          <div class="schedule-time">9:10 - 9:35</div>
          <div class="schedule-content">
            <h3>Video-TT Challenge: Towards Advanced Video Reasoning and Understanding</h3>
            <div class="schedule-speaker">
              <a href="https://sites.google.com/view/video-tt-challenge" target="_blank">Yuhao Dong, Yuanhan Zhang, Ziwei Liu, and Teams</a>
            </div>
          </div>
          <div class="schedule-actions">
            <button class="btn btn-outline" onclick="toggleAbstract(this)">Abstract</button>
            <a href="assets/slides/video_tt_challenge.pdf" class="btn btn-primary" target="_blank">Slides</a>
          </div>
          <div class="abstract-content">
            <strong>Abstract:</strong> Join us for the exciting conclusion of the Video-TT Challenge, where we will announce results and recognize outstanding achievements in video reasoning and understanding. This session will showcase innovative approaches from participating teams and highlight breakthrough methodologies that advance the state-of-the-art in video analysis and temporal reasoning.
          </div>
        </div>

        <div class="schedule-item">
          <div class="schedule-time">9:35 - 10:10</div>
          <div class="schedule-content">
            <h3>Reasoning in Multimodal GUI Agents: An Exploration-Driven Perspective</h3>
            <div class="schedule-speaker">
              <a href="https://icoz69.github.io/" target="_blank">Chi Zhang</a>
            </div>
          </div>
          <div class="schedule-actions">
            <button class="btn btn-outline" onclick="toggleAbstract(this)">Abstract</button>
            <a href="assets/slides/chizhang.pdf" class="btn btn-primary" target="_blank">Slides</a>
          </div>
          <div class="abstract-content">
            <strong>Abstract:</strong> This talk explores how multimodal GUI agents can develop sophisticated reasoning capabilities through exploration-driven learning. We will discuss novel approaches to understanding user interfaces, spatial relationships, and interaction patterns, highlighting how agents can learn to navigate complex digital environments through systematic exploration and reasoning.
          </div>
        </div>

        <div class="schedule-item">
          <div class="schedule-time">10:10 - 10:45</div>
          <div class="schedule-content">
            <h3>Mathematical Reasoning in Visual Contexts</h3>
            <div class="schedule-speaker">
              <a href="https://web.cs.ucla.edu/~kwchang/" target="_blank">Kai-Wei Chang</a>
            </div>
          </div>
          <div class="schedule-actions">
            <button class="btn btn-outline" onclick="toggleAbstract(this)">Abstract</button>
            <a href="assets/slides/kai-wei-chang-math-reasoning.pdf" class="btn btn-primary" target="_blank">Slides</a>
          </div>
          <div class="abstract-content">
            <strong>Abstract:</strong> Mathematical reasoning presents unique challenges when combined with visual information. This talk will explore how vision-language models can be enhanced to solve mathematical problems that require understanding geometric relationships, interpreting charts and graphs, and reasoning about spatial configurations in mathematical contexts.
          </div>
        </div>

        <div class="schedule-item">
          <div class="schedule-time">10:45 - 11:20</div>
          <div class="schedule-content">
            <h3>Chain-of-Look Visual Reasoning</h3>
            <div class="schedule-speaker">
              <a href="https://cse.buffalo.edu/~jsyuan/" target="_blank">Junsong Yuan</a>
            </div>
          </div>
          <div class="schedule-actions">
            <button class="btn btn-outline" onclick="toggleAbstract(this)">Abstract</button>
            <a href="assets/slides/junsong.pdf" class="btn btn-primary" target="_blank">Slides</a>
          </div>
          <div class="abstract-content">
            <strong>Abstract:</strong> While multi-modal foundation models excel at describing images and answering simple questions, they still struggle at tasks requiring deliberate, step-by-step visual reasoning. We introduce Chain-of-Look, a new visual reasoning paradigm that addresses this weakness by modeling sequential visual understanding, leading to more accurate, robust, and explainable reasoning in challenging scenes.
          </div>
        </div>

        <div class="schedule-item">
          <div class="schedule-time">11:20 - 11:55</div>
          <div class="schedule-content">
            <h3>Grounding Anything in Images and Videos for Comprehensive Reasoning</h3>
            <div class="schedule-speaker">
              <a href="https://faculty.ucmerced.edu/mhyang/" target="_blank">Ming-Hsuan Yang</a>
            </div>
          </div>
          <div class="schedule-actions">
            <button class="btn btn-outline" onclick="toggleAbstract(this)">Abstract</button>
            <a href="assets/slides/minghsuan.pdf" class="btn btn-primary" target="_blank">Slides</a>
          </div>
          <div class="abstract-content">
            <strong>Abstract:</strong> Understanding and reasoning about the visual world requires more than recognizing objects—it demands grounding language, actions, and abstract concepts in images and videos in a unified and interpretable way. This talk explores the emerging paradigm of comprehensive grounding, which bridges perception and reasoning by linking every visual element to structured representations.
          </div>
        </div>

        <div class="schedule-item">
          <div class="schedule-time">11:55 - 12:00</div>
          <div class="schedule-content">
            <h3>Closing Remark</h3>
            <div class="schedule-speaker">
              <a href="https://wangywust.github.io/" target="_blank">Yiwei Wang</a>
            </div>
          </div>
          <div class="schedule-actions">
            <button class="btn btn-outline" onclick="toggleAbstract(this)">Abstract</button>
            <a href="assets/slides/closing-remark.pdf" class="btn btn-primary" target="_blank">Slides</a>
          </div>
          <div class="abstract-content">
            <strong>Abstract:</strong> Our tutorial concludes with a synthesis of key insights, future research directions, and practical takeaways. This closing session will summarize the day's discussions, highlight emerging trends in vision-language reasoning, and provide guidance for researchers and practitioners looking to advance this exciting field.
          </div>
        </div>
      </div>
    </div>
  </section>

  <!-- Organizers -->
  <section id="organizers">
    <div class="container">
      <div class="section-header fade-in">
        <h2 class="section-title">Organizers</h2>
        <p class="section-subtitle">Leading Experts in Vision-Language Research</p>
      </div>
      
      <div class="people-grid">
        <div class="person-card fade-in">
          <div class="person-header">
            <img src="assets/images/avatars/yujuncai.png" alt="Yujun Cai" class="person-avatar">
            <div class="person-info">
              <h3>Yujun Cai</h3>
              <p class="person-affiliation">Lecturer at University of Queensland</p>
            </div>
          </div>
          <p class="person-bio">Dr. Yujun Cai is a Lecturer (Assistant Professor) in the University of Queensland, Australia. Previously, she was a Research Scientist at Meta Reality Labs in Seattle. She obtained her Ph.D. degree from Nanyang Technological University in 2021. Her research interests lie in multi-modal understanding and trust-worthy large models.</p>
          <div class="person-links">
            <a href="https://vanoracai.github.io/" target="_blank"><i class="fa fa-house"></i></a>
            <a href="https://scholar.google.com/citations?user=TE7lbQwAAAAJ" target="_blank"><i class="fas fa-graduation-cap"></i></a>
            <a href="https://github.com/vanoracai" target="_blank"><i class="fab fa-github"></i></a>
          </div>
        </div>

        <div class="person-card fade-in">
          <div class="person-header">
            <img src="assets/images/avatars/junliu.jpg" alt="Jun Liu" class="person-avatar">
            <div class="person-info">
              <h3>Jun Liu</h3>
              <p class="person-affiliation">Professor at Lancaster University</p>
            </div>
          </div>
          <p class="person-bio">Dr. Jun Liu is Professor and Chair in Digital Health at the School of Computing and Communications, Lancaster University. He earned his PhD from Nanyang Technological University in 2019, subsequently serving as faculty at Singapore University of Technology and Design from 2019 to 2024.</p>
          <div class="person-links">
            <a href="https://wp.lancs.ac.uk/vl/" target="_blank"><i class="fa fa-house"></i></a>
            <a href="https://scholar.google.com/citations?user=Q5Ild8UAAAAJ" target="_blank"><i class="fas fa-graduation-cap"></i></a>
            <a href="https://github.com/junliu" target="_blank"><i class="fab fa-github"></i></a>
          </div>
        </div>

        <div class="person-card fade-in">
          <div class="person-header">
            <img src="assets/images/avatars/yiweiwang.jpg" alt="Yiwei Wang" class="person-avatar">
            <div class="person-info">
              <h3>Yiwei Wang</h3>
              <p class="person-affiliation">Assistant Professor at UC Merced</p>
            </div>
          </div>
          <p class="person-bio">Dr. Yiwei Wang was an Applied Scientist in Amazon (Seattle) in 2023 and a Postdoc in UCLA NLP Group in 2024. He obtained his Ph.D. degree from National University of Singapore in 2023. Currently, he leads the UC Merced NLP Lab, where his team explores cutting-edge approaches to diffusion llms, reasoning multi-modal llms, and their applications.</p>
          <div class="person-links">
            <a href="https://wangywust.github.io/" target="_blank"><i class="fa fa-house"></i></a>
            <a href="https://scholar.google.com/citations?user=Sh9QvBkAAAAJ" target="_blank"><i class="fas fa-graduation-cap"></i></a>
            <a href="https://github.com/wangywust" target="_blank"><i class="fab fa-github"></i></a>
          </div>
        </div>

        <div class="person-card fade-in">
          <div class="person-header">
            <img src="assets/images/avatars/minghsuanyang.jpg" alt="Ming-Hsuan Yang" class="person-avatar">
            <div class="person-info">
              <h3>Ming-Hsuan Yang</h3>
              <p class="person-affiliation">Professor at UC Merced</p>
            </div>
          </div>
          <p class="person-bio">Ming-Hsuan Yang is a Professor in Electrical Engineering and Computer Science at University of California, Merced. He is a Fellow of the IEEE, ACM and AAAI.</p>
          <div class="person-links">
            <a href="https://faculty.ucmerced.edu/mhyang/" target="_blank"><i class="fa fa-house"></i></a>
            <a href="https://scholar.google.com/citations?user=p9-ohHsAAAAJ" target="_blank"><i class="fas fa-graduation-cap"></i></a>
            <a href="https://github.com/mhyang" target="_blank"><i class="fab fa-github"></i></a>
          </div>
        </div>
      </div>
    </div>
  </section>

  <!-- Speakers -->
  <section id="speakers">
    <div class="container">
      <div class="section-header fade-in">
        <h2 class="section-title">Invited Speakers</h2>
        <p class="section-subtitle">World-Class Researchers Sharing Their Insights</p>
      </div>
      
      <div class="people-grid">
        <div class="person-card fade-in">
          <div class="person-header">
            <img src="assets/images/avatars/kaiweichang.jpg" alt="Kai-Wei Chang" class="person-avatar">
            <div class="person-info">
              <h3>Kai-Wei Chang</h3>
              <p class="person-affiliation">Associate Professor at UCLA</p>
            </div>
          </div>
          <p class="person-bio">Dr. Kai-Wei Chang is an Associate Professor at UCLA specializing in natural language processing, machine learning, and multimodal reasoning. His research particularly focuses on mathematical reasoning, structured prediction, and developing robust AI systems that can handle complex reasoning tasks.</p>
          <div class="person-links">
            <a href="https://web.cs.ucla.edu/~kwchang/" target="_blank"><i class="fa fa-house"></i></a>
            <a href="https://scholar.google.com/citations?user=fqDBtzYAAAAJ" target="_blank"><i class="fas fa-graduation-cap"></i></a>
            <a href="https://github.com/kwchang" target="_blank"><i class="fab fa-github"></i></a>
          </div>
        </div>

        <div class="person-card fade-in">
          <div class="person-header">
            <img src="assets/images/avatars/junsongyuan.jpg" alt="Junsong Yuan" class="person-avatar">
            <div class="person-info">
              <h3>Junsong Yuan</h3>
              <p class="person-affiliation">Professor at University at Buffalo</p>
            </div>
          </div>
          <p class="person-bio">Dr. Junsong Yuan is a Professor at University at Buffalo, specializing in computer vision, pattern recognition, and multimedia analysis. His research encompasses video understanding, human activity recognition, and multimodal learning with applications in surveillance, healthcare, and autonomous systems.</p>
          <div class="person-links">
            <a href="https://cse.buffalo.edu/~jsyuan/" target="_blank"><i class="fa fa-house"></i></a>
            <a href="https://scholar.google.com/citations?user=fJ7seq0AAAAJ" target="_blank"><i class="fas fa-graduation-cap"></i></a>
            <a href="https://github.com/jsyuan" target="_blank"><i class="fab fa-github"></i></a>
          </div>
        </div>

        <div class="person-card fade-in">
          <div class="person-header">
            <img src="assets/images/avatars/ziweiliu.png" alt="Ziwei Liu" class="person-avatar">
            <div class="person-info">
              <h3>Ziwei Liu</h3>
              <p class="person-affiliation">Associate Professor at NTU</p>
            </div>
          </div>
          <p class="person-bio">Dr. Ziwei Liu is an Associate Professor at NTU and leads the LMMs-Lab initiative. His research focuses on large multimodal models, computer vision, and machine learning. He has made significant contributions to visual understanding, generative models, and multimodal intelligence systems.</p>
          <div class="person-links">
            <a href="https://liuziwei7.github.io/" target="_blank"><i class="fa fa-house"></i></a>
            <a href="https://scholar.google.com/citations?user=lc45xlcAAAAJ" target="_blank"><i class="fas fa-graduation-cap"></i></a>
            <a href="https://github.com/liuziwei7" target="_blank"><i class="fab fa-github"></i></a>
          </div>
        </div>

        <div class="person-card fade-in">
          <div class="person-header">
            <img src="assets/images/avatars/chizhang.jpg" alt="Chi Zhang" class="person-avatar">
            <div class="person-info">
              <h3>Chi Zhang</h3>
              <p class="person-affiliation">Assistant Professor at Westlake University</p>
            </div>
          </div>
          <p class="person-bio">Dr. Chi Zhang is an Assistant Professor at Westlake University, focusing on multimodal AI and embodied intelligence. His research spans GUI automation, visual reasoning, and human-computer interaction, with particular expertise in developing intelligent agents that can understand and interact with digital interfaces.</p>
          <div class="person-links">
            <a href="https://icoz69.github.io/" target="_blank"><i class="fa fa-house"></i></a>
            <a href="https://scholar.google.com/citations?user=J4s398EAAAAJ" target="_blank"><i class="fas fa-graduation-cap"></i></a>
            <a href="https://github.com/icoz69" target="_blank"><i class="fab fa-github"></i></a>
          </div>
        </div>

        <div class="person-card fade-in">
          <div class="person-header">
            <img src="assets/images/avatars/yuhaodong.jpg" alt="Yuhao Dong" class="person-avatar">
            <div class="person-info">
              <h3>Yuhao Dong</h3>
              <p class="person-affiliation">Ph.D. Student at NTU</p>
            </div>
          </div>
          <p class="person-bio">Yuhao Dong currently working on the topic of multimodal learning and his long-term goal is to build general foundation models. Recently he is focusing on vision-language model and unified visual models.</p>
          <div class="person-links">
            <a href="https://github.com/dongyh20" target="_blank"><i class="fa fa-house"></i></a>
            <a href="https://scholar.google.com/citations?user=kMui170AAAAJ" target="_blank"><i class="fas fa-graduation-cap"></i></a>
            <a href="https://github.com/dongyh20" target="_blank"><i class="fab fa-github"></i></a>
          </div>
        </div>

        <div class="person-card fade-in">
          <div class="person-header">
            <img src="assets/images/avatars/yuanhanzhang.jpg" alt="Yuanhan Zhang" class="person-avatar">
            <div class="person-info">
              <h3>Yuanhan Zhang</h3>
              <p class="person-affiliation">Ph.D. Student at NTU</p>
            </div>
          </div>
          <p class="person-bio">Yuanhan Zhang's research interests lie in computer vision and deep learning. His work focuses on adapting foundation models—from vision to multimodal—for real-world applications, including benchmarking model performance and adapting models through parameter-efficient tuning, in-context learning, and instruction tuning.</p>
          <div class="person-links">
            <a href="https://zhangyuanhan-ai.github.io/" target="_blank"><i class="fa fa-house"></i></a>
            <a href="https://scholar.google.com/citations?user=g6grFy0AAAAJ" target="_blank"><i class="fas fa-graduation-cap"></i></a>
            <a href="https://github.com/zhangyuanhan" target="_blank"><i class="fab fa-github"></i></a>
          </div>
        </div>
      </div>
    </div>
  </section>

  <!-- Gallery -->
  <section id="gallery">
    <div class="container">
      <div class="section-header fade-in">
        <h2 class="section-title">Gallery</h2>
        <p class="section-subtitle">Moments from the Tutorial</p>
      </div>
      
      <div class="gallery-grid" id="galleryGrid"></div>
    </div>
  </section>

  <!-- Lightbox -->
  <div class="lightbox" id="lightbox">
    <span class="lightbox-close" onclick="closeLightbox()">&times;</span>
    <span class="lightbox-nav lightbox-prev" onclick="changeImage(-1)">&#10094;</span>
    <img src="" alt="Gallery Image" id="lightboxImg">
    <span class="lightbox-nav lightbox-next" onclick="changeImage(1)">&#10095;</span>
  </div>

  <!-- Footer -->
  <footer>
    <div class="container">
      <p>&copy; 2025 Towards Comprehensive Reasoning in Vision-Language Models | ICCV Tutorial</p>
    </div>
  </footer>

  <script>
    // Navigation scroll effect
    const nav = document.getElementById('nav');
    window.addEventListener('scroll', () => {
      if (window.scrollY > 50) {
        nav.classList.add('scrolled');
      } else {
        nav.classList.remove('scrolled');
      }
    });

    // Mobile menu
    const menuToggle = document.getElementById('menuToggle');
    const navLinks = document.getElementById('navLinks');

    menuToggle.addEventListener('click', () => {
      menuToggle.classList.toggle('active');
      navLinks.classList.toggle('active');
    });

    // Close menu when clicking a link
    document.querySelectorAll('.nav-links a').forEach(link => {
      link.addEventListener('click', () => {
        menuToggle.classList.remove('active');
        navLinks.classList.remove('active');
      });
    });

    // Smooth scroll
    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
      anchor.addEventListener('click', function (e) {
        e.preventDefault();
        const target = document.querySelector(this.getAttribute('href'));
        if (target) {
          window.scrollTo({
            top: target.offsetTop - 80,
            behavior: 'smooth'
          });
        }
      });
    });

    // Abstract toggle
    function toggleAbstract(btn) {
      const item = btn.closest('.schedule-item');
      const abstract = item.querySelector('.abstract-content');
      abstract.classList.toggle('active');
      btn.textContent = abstract.classList.contains('active') ? 'Hide' : 'Abstract';
    }

    // Scroll animations
    const observerOptions = {
      threshold: 0.1,
      rootMargin: '0px 0px -50px 0px'
    };

    const observer = new IntersectionObserver((entries) => {
      entries.forEach(entry => {
        if (entry.isIntersecting) {
          entry.target.classList.add('visible');
        }
      });
    }, observerOptions);

    document.querySelectorAll('.fade-in').forEach(el => observer.observe(el));

    // Gallery
    const galleryFolder = 'assets/gallery/';
    const photoFiles = Array.from({length: 31}, (_, i) => `photo${i + 1}.jpg`);
    const galleryGrid = document.getElementById('galleryGrid');
    let currentImageIndex = 0;

    photoFiles.forEach((photo, index) => {
      const item = document.createElement('div');
      item.className = 'gallery-item fade-in';
      item.innerHTML = `<img src="${galleryFolder}${photo}" alt="Gallery Photo ${index + 1}">`;
      item.addEventListener('click', () => openLightbox(index));
      galleryGrid.appendChild(item);
      observer.observe(item);
    });

    function openLightbox(index) {
      currentImageIndex = index;
      document.getElementById('lightboxImg').src = galleryFolder + photoFiles[index];
      document.getElementById('lightbox').classList.add('active');
      document.body.style.overflow = 'hidden';
    }

    function closeLightbox() {
      document.getElementById('lightbox').classList.remove('active');
      document.body.style.overflow = '';
    }

    function changeImage(direction) {
      currentImageIndex = (currentImageIndex + direction + photoFiles.length) % photoFiles.length;
      document.getElementById('lightboxImg').src = galleryFolder + photoFiles[currentImageIndex];
    }

    // Keyboard navigation
    document.addEventListener('keydown', (e) => {
      const lightbox = document.getElementById('lightbox');
      if (!lightbox.classList.contains('active')) return;
      if (e.key === 'Escape') closeLightbox();
      if (e.key === 'ArrowRight') changeImage(1);
      if (e.key === 'ArrowLeft') changeImage(-1);
    });

    // Close lightbox on background click
    document.getElementById('lightbox').addEventListener('click', (e) => {
      if (e.target.id === 'lightbox') closeLightbox();
    });
  </script>
</body>
</html>